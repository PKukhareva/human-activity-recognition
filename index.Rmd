---
title: "Human Activity Recognition"
author: "Polina Kukhareva"
date: "July 19, 2018"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

Six participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of this project is to predict the manner in which these participants did the exercise. To achieve this goal, we developed three predictive models and compared their accuracy on the training data set. 54 numeric variables (features) were used to train the models. The best model (random forest) was used for final predictions on the testing data set and correctly classified all 20 test cases.


#Data Cleaning and Feature Selection

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 

We removed all factor variables, row numbers and timestamps. After that there remained 54 numeric variables.
We saved 30% of the dataset to test out of sample error.

```{r data}
library(caret)
#loading data
training_all = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")

#exploratory analysis
  #names(training_all)
  #summary(training_all)
table(training_all$classe)

#removing variables where over 30% is missing
training_all <- training_all[colSums(is.na(training_all))/nrow(training_all) < .3]

#removing all factor variables, row numbers and timestamps
classe_vector <- training_all$classe
nums <- unlist(lapply(training_all, is.numeric))  
training_all <- training_all[ , nums]
training_all <- training_all[ , colnames(training_all)!="X"]
training_all <- training_all[ , colnames(training_all)!="raw_timestamp_part_1"]
training_all <- training_all[ , colnames(training_all)!="raw_timestamp_part_2"]

#adding classe back
training_all$classe <- classe_vector

#data partitioning
inTrain = createDataPartition(training_all$classe, p = 0.7)[[1]]
training = training_all[ inTrain,]
validation = training_all[-inTrain,]
```

#Running Three Different Algorythms

We tried three predictive algorythms: Random Forest (RF), Linear Discriminant Analysis (LDA) and Classification and Regression Trees (CART). For each algorythm we used 54 available numeric variables (features). The algorythms were run with 3-fold cross validation to check the accuracy. Models we trained using caret package.

```{r models}
# Run algorithms using 3-fold cross validation
control <- trainControl(method="cv", number=3)
metric <- "Accuracy"
##Random Forest (RF)
set.seed(7)
fit.rf <- train(classe~., data=training, method="rf", na.action = na.pass, metric=metric, trControl=control)
##Linear Discriminant Analysis (LDA)
set.seed(7)
fit.lda <- train(classe~., data=training, method="lda", metric=metric, trControl=control)
##Classification and Regression Trees (CART).
set.seed(7)
fit.cart <- train(classe~., data=training, method="rpart", metric=metric, trControl=control)
```

#Accuracy of Predictive Models Estimated by the Cross Validation

Random forest had the best accuracy.

```{r results}
##summarize accuracy of models
results <- resamples(list(lda=fit.lda, rf=fit.rf, cart=fit.cart))
summary(results)

# compare accuracy of models
dotplot(results)
```

#Out of Sample Error

We estimated accuracy of the RF model on the validation dataset. 
```{r validation}
validation_predictions <- predict(fit.rf, validation)
confusionMatrix(validation_predictions, validation$classe)
```

#Classifying 20 Test Cases

Random forest showed the best accuracy, so we applied it for predicting test values. All predictions were correct.

```{r predict}
#predictions <- predict(fit.lda, testing)
predictions <- predict(fit.rf, testing)
#predictions <- predict(fit.cart, testing)
predictions
```


